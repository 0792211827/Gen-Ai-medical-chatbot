{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader  # Loads documents from PDFs and directories\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Splits text into smaller chunks for processing\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Embeds text into vectors\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # Generates text\n",
    "from langchain.chains import create_retrieval_chain # Creates a chain of processing steps\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain # Creates a chain of processing steps\n",
    "from langchain_core.prompts import ChatPromptTemplate # Creates a chain of processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text data from all PDF files in a specified directory\n",
    "def load_pdf_file(data):\n",
    "    # Initialize a DirectoryLoader to scan the directory for PDF files\n",
    "    loader = DirectoryLoader(\n",
    "        data,               # Path to the directory containing PDF files\n",
    "        glob=\"*.pdf\",       # Pattern to match only PDF files\n",
    "        loader_cls=PyPDFLoader  # Specify PyPDFLoader as the file loader\n",
    "    )\n",
    "\n",
    "    # Load all matching PDF files and extract their contents as documents\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents  # Return the extracted documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='/Users/sylviabhoke/Downloads/personal_repos folder/Gen-Ai-medical-chatbot/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks: 5860\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to split extracted text data into smaller chunks\n",
    "def text_split(extracted_data):\n",
    "    \n",
    "    \n",
    "    # Initialize a RecursiveCharacterTextSplitter to split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Maximum size of each chunk (in characters)\n",
    "        chunk_overlap=20  # Overlap between chunks to maintain context\n",
    "    )\n",
    "    \n",
    "    # Split the extracted documents into text chunks\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks  # Return the list of text chunks\n",
    "\n",
    "# Call the text_split function to process extracted data\n",
    "text_chunks = text_split(extracted_data)\n",
    "\n",
    "# Print the total number of text chunks generated\n",
    "print(\"Length of Text Chunks:\", len(text_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gq/r5lrhxfj6131xv1plhcsncym0000gn/T/ipykernel_6058/638705815.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "/Users/sylviabhoke/opt/anaconda3/envs/medibot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to download pre-trained embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    \"\"\"\n",
    "    Downloads the 'all-MiniLM-L6-v2' sentence-transformer model from Hugging Face \n",
    "    and initializes it for generating text embeddings.\n",
    "\n",
    "    Returns:\n",
    "        embeddings: An instance of HuggingFaceEmbeddings to generate vector embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the sentence transformer model from Hugging Face for embedding generation\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    return embeddings  # Return the initialized embeddings model\n",
    "\n",
    "# Call the function to download and initialize the embeddings model\n",
    "embeddings = download_hugging_face_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "import os  # Import os for handling environment variables\n",
    "\n",
    "# Initialize the Pinecone client using the API key (ensure it's stored securely)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Define the name of the index to be created\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "# Create a new Pinecone index with the specified configuration\n",
    "pc.create_index(\n",
    "    name=index_name,   # Name of the index\n",
    "    dimension=384,     # Dimensionality of the vector embeddings (should match the model's output dimension)\n",
    "    metric=\"cosine\",   # Similarity metric to use (options: \"cosine\", \"euclidean\", \"dotproduct\")\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",    # Cloud provider where the index will be hosted\n",
    "        region=\"us-east-1\"  # Specific region for hosting the index\n",
    "    ) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PineconeVectorStore module from LangChain's Pinecone integration\n",
    "from langchain_pinecone import PineconeVectorStore  \n",
    "\n",
    "# Embed each text chunk and upsert the embeddings into the specified Pinecone index\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,  # The list of text chunks to be embedded\n",
    "    index_name=index_name,  # The name of the Pinecone index where embeddings will be stored\n",
    "    embedding=embeddings,   # The embedding function/model used to generate vector representations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x116c06fe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "\n",
    "# Load an existing Pinecone index instead of creating a new one\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,  # The name of the pre-existing Pinecone index to connect to\n",
    "    embedding=embeddings    # The embedding model/function used for querying the stored vectors\n",
    ")\n",
    "\n",
    "# Display the loaded Pinecone index object\n",
    "docsearch  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fa393e7b-98d3-4246-bfe4-29d2a7129c85', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/sylviabhoke/Downloads/personal_repos folder/Gen-Ai-medical-chatbot/data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='c72fbe4d-780e-4a8d-b1a1-4f2792e607c3', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 39.0, 'page_label': '40', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/sylviabhoke/Downloads/personal_repos folder/Gen-Ai-medical-chatbot/data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 226\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 26'),\n",
       " Document(id='80b3fccf-7c7e-42a9-a744-5957d7b8d13c', metadata={'creationdate': '2004-12-18T17:00:02-05:00', 'creator': 'PyPDF', 'moddate': '2004-12-18T16:15:31-06:00', 'page': 38.0, 'page_label': '39', 'producer': 'PDFlib+PDI 5.0.0 (SunOS)', 'source': '/Users/sylviabhoke/Downloads/personal_repos folder/Gen-Ai-medical-chatbot/data/Medical_book.pdf', 'total_pages': 637.0}, page_content='GALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed.(Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing the retriever\n",
    "retrieved_docs = retriever.invoke(\"What is Acne?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # Use correct model name\n",
    "    google_api_key=os.getenv(\"GEMINI_API_KEY\"),  # Get key from .env\n",
    "    temperature=0.4,\n",
    "    max_output_tokens=500,\n",
    "    transport=\"rest\"  # <-- THIS is the magic fix!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template for the chatbot\n",
    "# The template includes a system prompt for providing context to the model\n",
    "# and a human prompt for accepting user input\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question-answering chain using the provided language model (LLM) and prompt.\n",
    "question_answer_chain = create_stuff_documents_chain(\n",
    "    llm,   # The language model used for generating responses\n",
    "    prompt # The prompt that guides how the model should process retrieved documents\n",
    ")\n",
    "\n",
    "# Create a Retrieval-Augmented Generation (RAG) chain\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever,             # The retriever that fetches relevant documents from the knowledge base\n",
    "    question_answer_chain  # The Q&A chain responsible for generating responses based on retrieved docs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acromegaly is a disorder caused by the abnormal release of a chemical from the pituitary gland.  This leads to increased bone and soft tissue growth, along with other bodily disturbances.  Gigantism is not explicitly defined in the provided text.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the RAG (Retrieval-Augmented Generation) chain with a user query\n",
    "response = rag_chain.invoke({\n",
    "    \"input\": \"What is Acromegaly and Gigantism?\"  # The query being asked\n",
    "})\n",
    "\n",
    "# Extract and print the answer from the response dictionary\n",
    "print(response[\"answer\"])  # Outputs the generated response based on retrieved documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, but I do not know what \"stats\" refers to in this context.  More information is needed.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is stats?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
